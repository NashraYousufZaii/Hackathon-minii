<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="indroduction.css">
    <title>Document</title>
</head>
<body>
    <div class="header">
        <a href="#default" class="logo">Personal Blogging app</a>
        <div class="header-right">
          <a href="login.html">login</a>
        </div>
      </div>
      <a href="allblogs.html">
        <h1 class="blog">< Back to all blogs</h1></a><br>
      <div>
        <h2>All from Elon Musk</h2><br>
    </div>
    <div class="dashboardBoarder">
        <p><img src="profile 2.png" alt="Bird" width="90"
            height="90" align="left" /> <h3>Introducing Whisper</h3>
           <p>Elon Musk - August 17th, 2023</p>
           </p>
          <div class="one">
           <p>Whisper is an automatic speech recognition (ASR) system trained on 680,000 hours of multilingual and  <br> multitask supervised data collected from the web. We show that the use of such a large and diverse dataset <br>  leads to improved robustness to accents,  background noise and technical language. Moreover, it enables  <br> transcription in multiple languages, as well as translation from those languages into English. We are open- <br> sourcing models and inference code to serve as a foundation for building useful applications and for further <br> research on robust speech processing.</p> 
       
        </div>
           </div>

           <div class="dashboardBoarder">
            <p><img src="profile 2.png" alt="Bird" width="90"
                height="90" align="left" /> <h3>Introducing Whisper</h3>
               <p>Elon Musk - November , 2022</p>
               </p>
               <div><p>We’ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it <br> possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and <br>reject inappropriate requests.
                We’ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it<br> possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and <br>reject inappropriate requests.<br> <br>Methods:<br><br>
                We trained this model using Reinforcement Learning from Human Feedback (RLHF), using the same methods <br>as InstructGPT, but with slight differences in the data collection setup. We trained an initial model using<br> supervised fine-tuning: human AI trainers provided conversations in which they played both sides—the user<br> and an AI assistant. We gave the trainers access to model-written suggestions to help them compose their<br> responses. We mixed this new dialogue dataset with the InstructGPT dataset, which we transformed into a <br>dialogue format.
                To create a reward model for reinforcement learning, we needed to collect comparison data, which consisted <br>of two or more model responses ranked by quality. To collect this data, we took conversations that AI trainers <br>had with the chatbot. We randomly selected a model-written message, sampled several alternative<br> completions, and had AI trainers rank them. Using these reward models, we can fine-tune the model using <br>Proximal Policy Optimization. We performed several iterations of this process.
                </p>
            </div>
               </div>      
</body>
</html>